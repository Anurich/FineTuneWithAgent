# Project Title: Feedback-Guided LLM Training

## Overview

This project implements a system for training a Large Language Model (LLM) using a feedback-guided approach. It involves two main LLMs:

1.  **Data Generation LLM (gpt-4.1-mini)**: Generates question-reasoning-answer triplets.
2.  **Training LLM (unsloth/Qwen3-4B-Base)**: Fine-tuned on the generated data.

The core idea is to iteratively generate data, train the model, evaluate its performance, and use the evaluation feedback to guide subsequent data generation, aiming to improve the model's capabilities in specific domains (e.g., mathematics).

## Project Structure

The project is organized as follows:

```
.
├── README.md                   # This file.
├── feedback_augmented_loss_example.pdf # Example PDF (likely related to loss calculation)
├── noname.pdf                  # Another PDF, purpose unknown from filename
├── src/                        # Source code directory
│   ├── .env                    # Environment variables (e.g., API keys)
│   ├── Agent/                  # Contains code for the agents
│   │   ├── creator_agent/      # Code for the data generation agent
│   │   │   └── create.py       # Defines FeedbackGuidedAgentA for creating question-answer pairs
│   │   └── evaluation_agent/   # Code for the evaluation agent
│   │       └── evaluate.py     # Defines the Evaluate class for assessing model performance
│   ├── all_generated_questions.json # Stores all questions generated by the creator agent
│   ├── main.py                 # Main script to run the training loop
│   └── model/                  # Contains code for the training model
│       └── train.py            # Defines the MODEL class and custom SFTTrainer for fine-tuning
└── training_results.json       # Stores results from training iterations (created after running main.py)
```

### Key Components:

*   **`src/main.py`**: This is the entry point of the application. It orchestrates the data generation, model training, and evaluation loop.
*   **`src/Agent/creator_agent/create.py`**: Defines `FeedbackGuidedAgentA`. This agent uses `gpt-4.1-mini` to generate educational content (question-reasoning-answer triplets). It can adapt its generation strategy based on feedback from the model's performance.
*   **`src/Agent/evaluation_agent/evaluate.py`**: Defines the `Evaluate` class. This component assesses the quality of the trained model's outputs by comparing them against ground truth answers. It provides scores for correctness and reasoning.
*   **`src/model/train.py`**: Contains the `MODEL` class, which encapsulates the `unsloth/Qwen3-4B-Base` model, and `FinalCorrectedSFTTrainer`, a custom trainer that incorporates the feedback from the evaluation agent to adjust the learning process.
*   **`src/.env`**: This file should store sensitive information like API keys for services such as OpenAI. **Note:** This file is crucial and should not be committed to public repositories.
*   **`all_generated_questions.json`**: A log of all question-answer pairs generated during the data creation phase.
*   **`training_results.json`**: This file is generated after running `main.py` and contains a summary of the training progress and performance metrics for each iteration.

## Setup and Usage

### Prerequisites

*   Python 3.8 or higher.
*   An OpenAI API key (for generating data using `gpt-4.1-mini`).
*   Access to a machine with a GPU is recommended for training the `unsloth/Qwen3-4B-Base` model.

### Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    The project uses several Python libraries. While a `requirements.txt` file is not explicitly provided in the listing, common libraries for such a project include:
    *   `langchain`
    *   `langchain-openai`
    *   `python-dotenv`
    *   `numpy`
    *   `torch`
    *   `transformers`
    *   `datasets`
    *   `trl`
    *   `unsloth`
    *   `pandas`

    You can install them manually:
    ```bash
    pip install langchain langchain-openai python-dotenv numpy torch transformers datasets trl unsloth pandas
    ```
    *(Ideally, a `requirements.txt` file should be created and used: `pip install -r requirements.txt`)*

4.  **Set up environment variables:**
    Create a file named `.env` in the `src/` directory and add your OpenAI API key:
    ```env
    OPENAI_API_KEY='your_openai_api_key_here'
    ```
    **Note:** Ensure `src/.env` is listed in your `.gitignore` file to prevent committing your API key.

### Running the Project

1.  **Navigate to the source directory:**
    ```bash
    cd src
    ```

2.  **Run the main training loop:**
    ```bash
    python main.py
    ```
    This will start the iterative process of data generation, model training, and evaluation. Training results and generated questions will be saved to `training_results.json` and `all_generated_questions.json` respectively.

### Expected Output Files

After running `main.py`, you can expect the following files to be created/updated:

*   **`src/all_generated_questions.json`**: Contains all the question-reasoning-answer triplets generated by the `FeedbackGuidedAgentA`. This file is appended to in each data generation cycle.
*   **`training_results.json`**: Located in the root directory, this file stores a summary of each training iteration, including performance metrics like quality EMA, reasoning scores, and solution scores.

## Feedback Mechanism

A key feature of this project is its iterative feedback loop, designed to continuously improve the training data and, consequently, the model's performance. This loop consists of several stages:

1.  **Data Generation**: The `FeedbackGuidedAgentA` (`src/Agent/creator_agent/create.py`) generates an initial set of question-reasoning-answer triplets. In subsequent iterations, this agent can use feedback from the `MODEL`'s performance to refine its generation strategy.

2.  **Model Training**: The `MODEL` (`src/model/train.py`), which is based on `unsloth/Qwen3-4B-Base`, is fine-tuned on the currently available generated data. The `FinalCorrectedSFTTrainer` is a custom trainer that can incorporate fine-grained feedback.

3.  **Evaluation**: The `Evaluate` class (`src/Agent/evaluation_agent/evaluate.py`) assesses the trained `MODEL`'s predictions. It compares the model's answers and reasoning against ground truth solutions (or a reference, if ground truth is not available for all generated data). This evaluation produces metrics such as:
    *   `solution_score`: Correctness of the final answer.
    *   `reasoning_score`: Quality and logical soundness of the reasoning steps.
    *   `is_correct`: A boolean indicating overall correctness.
    These scores are normalized using z-score statistics.

4.  **Feedback Integration**:
    *   **For Data Generation (`FeedbackGuidedAgentA`)**: The evaluation results, particularly the z-scores for reasoning and solution quality, and trends in performance (e.g., quality EMA), are fed back to the `FeedbackGuidedAgentA`. The agent uses these insights (detailed in `_analyze_training_performance` and `_generate_performance_insights` methods) to:
        *   Identify areas where the `MODEL` is weak (e.g., poor reasoning on certain types of problems).
        *   Adjust the difficulty, type, or focus of newly generated questions to target these weaknesses.
        *   Reinforce areas where the `MODEL` is performing well.
    *   **For Model Training (`FinalCorrectedSFTTrainer`)**: The custom trainer (`FinalCorrectedSFTTrainer` in `src/model/train.py`) uses the evaluation feedback (specifically, z-scores from `Evaluate`) *during* the training process for the current batch of data. It implements several z-score aware mechanisms:
        *   **Z-Score Aware Feedback**: It adjusts the loss computation based on the quality (reasoning and solution z-scores) of the model's predictions on the current batch.
        *   **Temperature Adjustment**: Modifies the softmax temperature based on z-scores. Higher z-scores (better quality) lead to lower temperatures (more confident predictions), while lower z-scores (poorer quality) lead to higher temperatures.
        *   **Confidence Penalty**: Applies a penalty if the model is overconfident on low-quality (low z-score) answers or underconfident on high-quality (high z-score) answers.
        *   **Gradient Scaling**: Scales gradients based on the magnitude of z-scores to emphasize learning from low-quality examples or temper learning from very high-quality ones.
        *   **Quality EMA**: Maintains an Exponential Moving Average (EMA) of overall quality to provide a smoothed, stable measure of performance over time, preventing drastic reactions to noisy, single-batch evaluations.

This iterative process allows the system to dynamically adapt its data generation and training strategies, aiming for more efficient and targeted model improvement. The `main_training_loop` in `src/main.py` orchestrates these steps over multiple iterations.

## Models Used

This project utilizes two primary Large Language Models (LLMs):

1.  **Data Generation Model: `gpt-4.1-mini`**
    *   **Purpose**: Used by the `FeedbackGuidedAgentA` (`src/Agent/creator_agent/create.py`) to generate question-reasoning-answer triplets.
    *   **Integration**: Accessed via the OpenAI API, likely using the `langchain-openai` library.
    *   **Role**: Acts as the "expert" or "teacher" that creates the initial educational content. Its ability to generate diverse and complex problems is crucial for the project's success. The quality of the data generated by this model directly impacts the upper bound of the trainee model's potential performance.

2.  **Fine-tuning Model: `unsloth/Qwen3-4B-Base`**
    *   **Purpose**: This is the model that is trained and improved upon using the generated data.
    *   **Integration**: Used within the `MODEL` class in `src/model/train.py`. The `unsloth` library is employed for efficient fine-tuning, including LoRA (Low-Rank Adaptation) for parameter-efficient training and 4-bit quantization (though 4-bit is specified as `load_in_4bit=False` in the current `MODEL` class, `unsloth` is known for such features).
    *   **Role**: Acts as the "student" model. It learns from the data provided by `gpt-4.1-mini` and is iteratively improved through the feedback mechanism. The goal is to make this model proficient in the domain it's being trained on (e.g., mathematics).

The interaction between these two models—one generating data and the other learning from it, with a feedback loop refining both processes—is central to the project's architecture.

## Contributing

Contributions to this project are welcome! If you're interested in improving the code, adding features, or fixing bugs, please consider the following:

1.  **Fork the Repository**: Start by forking the project to your own GitHub account.
2.  **Create a Branch**: Create a new branch for your changes (e.g., `feature/new-evaluation-metric` or `fix/readme-typo`).
3.  **Make Your Changes**: Implement your improvements or fixes.
4.  **Test Your Changes**: Ensure your changes don't break existing functionality. If applicable, add new tests.
5.  **Write Clear Commit Messages**: Follow standard commit message conventions.
6.  **Submit a Pull Request**: Push your changes to your fork and submit a pull request to the main repository, detailing the changes you've made.

If you have suggestions or want to discuss potential changes, feel free to open an issue.

## License

This project is currently unlicensed.

If you plan to distribute or share this code, it's recommended to choose a license. A common choice for open-source projects is the [MIT License](https://opensource.org/licenses/MIT), which is permissive and allows for broad use.

To add a license:
1. Create a `LICENSE` file in the root of your project.
2. Paste the text of your chosen license into this file.
3. Update this section in the `README.md` to reflect the chosen license (e.g., "This project is licensed under the MIT License - see the LICENSE file for details.").
